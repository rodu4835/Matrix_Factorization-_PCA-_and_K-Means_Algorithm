{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OvsIpputCaE5"
   },
   "source": [
    "# Final Project - Matrix Factorization, Principal Component Analysis and K-Means Algorithm\n",
    "## CSPB 2820 -  Linear Algebra with Computer Science Applications\n",
    "*Name*: $<$*Write your name here*$>$ \n",
    "\n",
    "This assignment is due on Moodle by *11:59PM on August 8th*.\n",
    "\n",
    "Submit only this Jupyter notebook to Moodle with the name format `Final_Project_<yourname>.ipynb`. Do not compress it using tar, rar, zip, etc. You must test this notebook in Jupyter Lab.\n",
    "Your solutions to analysis questions should be done in Markdown directly below the associated question. You can add a write-up markdown cell if it wasn't provided.\n",
    "\n",
    "All the written code must be commented as shown in this link https://realpython.com/python-comments-guide/ \n",
    "\n",
    "Remember that you are encouraged to discuss the problems with your classmates and instructors, \n",
    "but *you must write all code and solutions on your own*, and list any people or sources consulted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8xW7owYDhS7"
   },
   "source": [
    "# Elementary Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(15).reshape(3, 5)\n",
    "print(a)\n",
    "b = np.arange(15).reshape(5, 3)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJJPj-JAD1Pd"
   },
   "source": [
    "# Dot product of two arrays can be calculated using the numpy.dot() function from the numpy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nv_LIa3gDrG8",
    "outputId": "6531506f-f3c5-410c-9072-3f0f94ae7f2f"
   },
   "outputs": [],
   "source": [
    "print(np.dot(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJtxyULwD8Q9"
   },
   "source": [
    "# Problem 1 : What is a Matrix Factorization? [30 points]\n",
    "\n",
    "A matrix factorization is a way of reducing a matrix into its constituent parts.\n",
    "\n",
    "It is an approach that can simplify more complex matrix operations that can be performed on the factorized matrix rather than on the original matrix itself.\n",
    "\n",
    "NOTE: You can choose to implement any one of the two matrix factorization method (LU and Cholesky) and gain 7 marks each. QR Matrix Factorization and Singular-Value Decomposition are complusory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XM0XEbiERKf"
   },
   "source": [
    "# 1.1 : LU Matrix Factorization [5 points]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    The LU Factorization is for square matrices and Factorizes a matrix into L and U components.\n",
    "\n",
    "**A = L.U or LU**\n",
    "\n",
    "    Where A is the square matrix that we wish to Factorize, L is the lower triangle matrix and U is the upper triangle matrix.\n",
    "\n",
    "## LU Factorization with partial pivoting**\n",
    "\n",
    "    A variation of the LU Factorization that is numerically more stable to solve in practice.\n",
    "\n",
    "**A = P . L . U**\n",
    "\n",
    "    The rows of the parent matrix are re-ordered to simplify the Factorization process and the additional P matrix specifies a way to permute the result or return the result to the original order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyAR58CXET4K"
   },
   "source": [
    "Please refer to https://en.wikipedia.org/wiki/LU_decomposition for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from scipy.linalg import lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l6ZnChu8EOvq",
    "outputId": "04f294bf-7a07-4cc5-d70a-63749ca546bf"
   },
   "outputs": [],
   "source": [
    "# define a square matrix\n",
    "A = array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(A)\n",
    "\n",
    "# LU decomposition\n",
    "P, L, U = lu(A)\n",
    "print(P)\n",
    "print(L)\n",
    "print(U)\n",
    "\n",
    "# reconstruct\n",
    "B = P.dot(L).dot(U)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e3tS8xt_PN7"
   },
   "source": [
    "### Question 1 : Will the LU factorization without Pivoting work on both square and non-square matrices ? Explain with reasons. [1 points]\n",
    "\n",
    "### Answer : \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TxD--JGIgwb"
   },
   "source": [
    "Please refer to https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lu.html for more information on the LU library used above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYBcg_AjJJ1l"
   },
   "source": [
    "## Implement the LU factorization methodology from scratch and test it using the same example as above.\n",
    "  \n",
    "\n",
    "##   [1.11] Complete the Matrix_multiplication function. [1 Point]\n",
    "##   [1.12] Complete the Pivot_Calculation function. [1 Point]\n",
    "##   [1.13] Complete the LU_Factorizer function. [2 Points]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzZMmqDjIOkU"
   },
   "outputs": [],
   "source": [
    "class LU_Factorization:\n",
    "\n",
    "    def __init__(self, matrix = None):\n",
    "        \"\"\"\n",
    "        Create a LU Factorizer\n",
    "        :param matrix: Matrix that needs to be factorized\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.matrix = matrix \n",
    "\n",
    "    def Matrix_Multiplication(self, X, Y):\n",
    "        \"\"\"\n",
    "        Create a function to multiply 2 square matrices of same shape\n",
    "        :param X and Y: 2 matrices of shape (m,n) respectively that are to be multiplied.\n",
    "        :return: Matrix of shape (m,m)\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 1.11\n",
    "        # TO DO: Complete this function to return the multiplied matrix.\n",
    "        #BEGIN \n",
    "        # code here\n",
    "\n",
    "        # End\n",
    "\n",
    "    def Pivot_Calculation(self):\n",
    "        \"\"\"\n",
    "        Create a function to calculate the pivoting matrix for the given LU_Factorization object.\n",
    "        :return: Matrix of shape (m,n)\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 1.12\n",
    "        # TO DO: Complete this function to return the Pivot matrix.\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        \n",
    "        # End\n",
    "\n",
    "    def LU_Factorizer(self):\n",
    "        \"\"\"\n",
    "        Create a function to factorize matrix for the given LU_Factorization object.\n",
    "        :return: Return the Matrices P, L and U.\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 1.12\n",
    "        # TO DO: Complete this function to return the P, L and U matrices.\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        \n",
    "\n",
    "        # End\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.testLU(LU_Factorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BN3zzNqJQXvV"
   },
   "source": [
    "# 1.2 : QR Matrix Factorization [21 points]\n",
    "\n",
    "\n",
    "    The QR Factorization is for m x n matrices (not limited to square matrices) and decomposes a matrix into Q and R components.\n",
    "\n",
    "**A = Q . R or QR**\n",
    "\n",
    "    Where A is the matrix that we wish to Factorize, Q a matrix with the size m x m, and R is an upper triangle matrix with the size m x n.\n",
    "\n",
    "Please refer to https://en.wikipedia.org/wiki/QR_decomposition for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from scipy.linalg import qr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RaiTJfMeQYX3",
    "outputId": "d8906bae-3d26-4e28-98a1-77589de48bef"
   },
   "outputs": [],
   "source": [
    "# QR decomposition\n",
    "# define a 3x2 matrix\n",
    "A = array([[60, 91, 26], [60, 3, 75], [45, 90, 31]])\n",
    "D = np.diag(np.sign(np.diag(A)))\n",
    "print(A)\n",
    "\n",
    "# QR decomposition\n",
    "Q_intermediate, R_intermediate = qr(A)\n",
    "Q_diagonal = np.diag(np.sign(np.diag(Q_intermediate)))\n",
    "Q = np.matmul(Q_intermediate, Q_diagonal)\n",
    "R = np.matmul(Q_diagonal,R_intermediate)\n",
    "\n",
    "print(Q)\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1p4zvm9QurI"
   },
   "source": [
    "Please refer to https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.qr.html for more information on the QR library used above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8gMEPBqbR_u"
   },
   "source": [
    "## Please implement the QR factorization methodology from scratch and test it using the same example as above.\n",
    "  \n",
    "\n",
    "##   [1.21] Complete the Norm_Calculation function. [3 Points]\n",
    "##   [1.22] Complete the Matrix_Multiplication function. [4 Points]\n",
    "##   [1.23] Complete the QR_Factorizer function. [12 Points]\n",
    "\n",
    "          Note - Do not implement the code given in the VMLS Python Companion book from Chapter 10 as it is.\n",
    "          Hint - Make use of the 1.21 and 1.22 functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWNGNeIHYXRr"
   },
   "outputs": [],
   "source": [
    "class QR_Factorization:\n",
    "\n",
    "    def __init__(self, matrix = None):\n",
    "        \"\"\"\n",
    "        Create a QR Factorizer\n",
    "        :param matrix: Matrix that needs to be factorized\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.matrix = matrix \n",
    "\n",
    "    def Norm_Calculation(self,A):\n",
    "        \"\"\"\n",
    "        Create a function to calculate the Euclidean norm of a matrix.\n",
    "        :param A: Matrix for which norm is to be calculated.\n",
    "        :return: Return a floating point Norm value.\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 1.21\n",
    "        # TO DO: Complete this function to return the Norm of a matrix.\n",
    "        #BEGIN \n",
    "        # code here\n",
    "                \n",
    "        # End\n",
    "\n",
    "    def Matrix_Multiplication(self, X, Y):\n",
    "        \"\"\"\n",
    "        Create a function to multiply 2 square matrices of same shape\n",
    "        :param X and Y: 2 matrices of shape (m,n) respectively that are to be multiplied.\n",
    "        :return: Matrix of shape (m,m)\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 1.22\n",
    "        # TO DO: Complete this function to return the multiplied matrix.\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        \n",
    "        # End\n",
    "\n",
    "    def QR_Factorizer(self):\n",
    "        \"\"\"\n",
    "        Create a function to factorize matrix for the given QR_Factorization object using Gram Schmidt Process.\n",
    "        :return: Return the Matrices Q and R.\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 1.23\n",
    "        # TO DO: Complete this function to return the Q and R matrices.\n",
    "        #BEGIN \n",
    "        # code here\n",
    "        \n",
    "        # End\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.testQR(QR_Factorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnE9Odvn_t6d"
   },
   "source": [
    "### Question 2 : Will the code you have written work on both square and non square matrices ? [2 points]\n",
    "\n",
    "### Answer :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmWFxuh8hsF8"
   },
   "source": [
    "# 1.3 : Cholesky Matrix Factorization [4 points]\n",
    "\n",
    "\n",
    "    The Cholesky Factorization is for square symmetric matrices where all eigenvalues are greater than zero, so-called positive definite matrices.\n",
    "\n",
    "**A = L . L^T**\n",
    "\n",
    "     Where A is the matrix being Factorized, L is the lower triangular matrix and L^T is the transpose of L.\n",
    "\n",
    "**A = U^T . U**\n",
    "\n",
    "    Where U is the upper triangular matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zo106y3aiJfI"
   },
   "source": [
    "Please refer to the https://en.wikipedia.org/wiki/Cholesky_decomposition for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from scipy.linalg import cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xe0RLkZXkPry",
    "outputId": "7a051700-0884-44f8-d1c9-8d723a6b1f58"
   },
   "outputs": [],
   "source": [
    "# Cholesky factorization\n",
    "# define a 3x3 matrix\n",
    "A = array([[2, 1, 1], [1, 2, 1], [1, 1, 2]])\n",
    "print(A)\n",
    "\n",
    "L = cholesky(A)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVOqlVvpATqG"
   },
   "source": [
    "### Question 3 : Provide examples for Cholesky Factorization using both Upper Triangular and Lower Triangualar matrices. [1 points]\n",
    "\n",
    "### Answer :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHAa3XUboKUL"
   },
   "source": [
    "Please refer to https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.cholesky.html for more information on the Cholesky library used above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxxIBoxEipe5"
   },
   "source": [
    "## Please implement the Cholesky factorization methodology from scratch and test it using the same example as above.\n",
    "  \n",
    "\n",
    "##  [1.3] Complete the Cholesky_Factorizer.[3 Points]\n",
    "\n",
    "          Note - Do not implement the code given in the VMLS Python Companion book as it is.\n",
    "          Hint - Make use of the 1.21 and 1.22 functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnSaU3LDkqAK"
   },
   "outputs": [],
   "source": [
    "class Cholesky_Factorization:\n",
    "\n",
    "    def __init__(self, matrix = None):\n",
    "        \"\"\"\n",
    "        Create a Cholesky Factorizer\n",
    "        :param matrix: Matrix that needs to be factorized\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.matrix = matrix \n",
    "\n",
    "\n",
    "    def Cholesky_Factorizer(self):\n",
    "        \"\"\"\n",
    "        Create a function to factorize matrix for the given Cholesky_Factorization object.\n",
    "        :return: Return the Matrix L or U.\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 1.31\n",
    "        # TO DO: Complete this function to return the Q and R matrices.\n",
    "        #BEGIN \n",
    "        # code here\n",
    "\n",
    "        # End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.testCholesky(Cholesky_Factorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8BHs7wymweA"
   },
   "source": [
    "# 1.4 : Singular-Value Decomposition [7 points]\n",
    "\n",
    "\n",
    "    The Singular-Value Decomposition, or SVD for short, is a matrix decomposition method for reducing a matrix to its constituent parts in order to make certain subsequent matrix calculations simpler.\n",
    "\n",
    "    For the case of simplicity we will focus on the SVD for real-valued matrices and ignore the case for complex numbers.\n",
    "\n",
    "**A = U . Sigma . V^T**\n",
    "\n",
    "    Where A is the real m x n matrix that we wish to decompose, U is an m x m matrix, Sigma (often represented by the uppercase Greek letter Sigma) is an m x n diagonal matrix, and V^T is the  transpose of an n x n matrix where T is a superscript.\n",
    "\n",
    "    The diagonal values in the Sigma matrix are known as the singular values of the original matrix A. The columns of the U matrix are called the left-singular vectors of A, and the columns of V are called the right-singular vectors of A.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQ4sb4DuAiFN"
   },
   "source": [
    "### Question 4 : Explain how we can find Pseudo inverse of a matrix using SVD. [2 points]\n",
    "\n",
    "### Answer :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LAo8UpvnH57"
   },
   "source": [
    "Please refer to the https://en.wikipedia.org/wiki/Singular_value_decomposition for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from scipy.linalg import svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dQhrZxOAmKaZ",
    "outputId": "fe5b6c35-173a-4f38-eee3-e7054535eaec"
   },
   "outputs": [],
   "source": [
    "# Singular-value decomposition\n",
    "# define a matrix\n",
    "A = array([[1, 2], [3, 4], [5, 6]])\n",
    "print(A)\n",
    "\n",
    "# SVD\n",
    "U, s, VT = svd(A)\n",
    "print(U)\n",
    "print(s)\n",
    "print(VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tchaT7aNqFhq"
   },
   "source": [
    "##   [1.4] Using the above U, s and VT matrices, construct the original A matrix.[5 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVUAVjVA1Pyd"
   },
   "outputs": [],
   "source": [
    "# Workspace 1.41\n",
    "#BEGIN \n",
    "# code here\n",
    "class SVD:\n",
    "\n",
    "    def __init__(self, U=None, Sigma=None, VT=None, shape=None):\n",
    "        \"\"\"\n",
    "        Create a Cholesky Factorizer\n",
    "        \"\"\"\n",
    "\n",
    "        self.U = U \n",
    "        self.Sigma = Sigma\n",
    "        self.VT = VT\n",
    "        self.shape = [3,2]\n",
    "\n",
    "    def Reconstruct_SVD(self):\n",
    "        \"\"\"\n",
    "        Create a function to reconstruct the original matrix using the 3 matrices generated by SVD.\n",
    "        :return: Return the Original Matrix.\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 1.41\n",
    "        # TO DO: Complete this function to return the original matrix.\n",
    "        #BEGIN \n",
    "        # code here\n",
    "     \n",
    "        # End\n",
    "\n",
    "\n",
    "#END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.testSVD(SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGn1CT7RvIkZ"
   },
   "source": [
    "# Problem 2 : K-means (49 points)\n",
    "\n",
    "The goal of K-means is to partition the data into $k$ clusters such that the sum of intra-cluster variances is minimal.\n",
    "\n",
    "In this problem, we'll be implementing K-means and evaluate it on the multi_blobs data shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7u8Bt-YOC43f"
   },
   "source": [
    "### Question 5 : What is the significance of K in K-Means algorithm ? [1 point]\n",
    "\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "NwSbUPByv1cP",
    "outputId": "9868be1f-40cc-4b2b-8edc-899aca9491bf"
   },
   "outputs": [],
   "source": [
    "# Do not modify this cell\n",
    "class DataBlobs:\n",
    "    def __init__(self, centers, std=1.75):\n",
    "        self.X, self.labels = make_blobs(n_samples=400, n_features=2, cluster_std=std, centers=centers,\n",
    "                                         shuffle=False, random_state=5622)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.labels,\n",
    "                                                                                test_size=0.3, random_state=5622)\n",
    "multi_blobs = DataBlobs(centers=5, std=1.5)\n",
    "\n",
    "plt.title(\"multi_blobs\")\n",
    "plt.scatter(multi_blobs.X[:, 0], multi_blobs.X[:, 1], c=multi_blobs.labels)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXwrcd3UvwPV"
   },
   "source": [
    "\n",
    "\n",
    "We will be using Euclidean distance as our variance measure, so for cluster $C_i = \\{x_1,x_2,... x_{m_i}\\}$, its intra-cluster variance $V(C_i)$ is defined as:\n",
    "\n",
    "$$\n",
    "V(C_i) = \\sum_{j=1}^{m_i} ||x_j - \\mu_i||^2\n",
    "$$\n",
    "\n",
    "where $\\mu_i = \\frac{1}{m} \\sum_{i=1}^{m_i} x_i$. $\\mu_i$ is called the centroid of cluster $C_i$.\n",
    "\n",
    "So for $k$ clusters, K-means objective is:\n",
    "$$\n",
    "\\min_{C_1,C_2\\ldots C_k}\\sum_{i=1}^{k}V(C_i) = \\min_{C_1,C_2\\ldots C_k} \\sum_{i=1}^{k} \\sum_{j=1}^{m_i} ||x_j - \\mu_i||^2\n",
    "$$\n",
    "\n",
    "Each sample $x_i$ is assigned to the cluster of the closest centroid. Hence, finding the optimal partition $\\{C_1,C_2...C_k\\}$ that minimizes the objective is equivalent to finding the optimal centroids.\n",
    "\n",
    "Unfortunately, there is no algorithm that reaches the global optima for K-means, but we'll be implementing the most famous heuristic for the problem: Llyod algorithm. It works as follows:\n",
    "\n",
    "- Initialize the centroids with **unique** random samples (`initialize_centroids`), initial objective = $+\\infty$\n",
    "- Repeat until convergence:\n",
    "    - Compute the distances matrix $D$ between samples and centroids (`compute_distances`)\n",
    "    - Use $D$ to assign each sample to the cluster with the closest centroid (`computes_assignments`)\n",
    "    - Update the centroids as centers of the new cluster assignments (`compute_centroids`)\n",
    "    - Compute the new objective (`compute_objective`)\n",
    "    - Stop if the improvement ratio of the objective is less than $\\epsilon$\n",
    "\n",
    "The improvement ratio equal to `|new_objective - previous_objective|/|previous_objective|`.\n",
    "\n",
    "\n",
    "- **1.1 [4 points]** `initialize_centroids` : select K **distinct** samples from the matrix data `X` and use them as the initial centroids. Store these centroids in the class attribute `self.centroids` as an `np.array` of shape $k \\times d$.\n",
    "- **1.2 [4 points]** `compute_distances` : compute the distance of each sample $x_i$ to every centroid $c_j$ and return the result as a matrix `distances_matrix` of size $N \\times k$ where `N` is the number of samples and `k` is the chosen number of clusters to be found. The cell `(i,k)` shall contain the euclidean distance between sample $x_i$ and centroid $m_k$.\n",
    "- **1.3 [4 points]** `compute_assignments` : given the distances matrix of size $N \\times k$ return an array of labels in which each element is an integer in the range $[0, k-1]$ and it represents which centroid it's closest to.\n",
    "- **1.4 [5 points]** `compute_centroids` : Compute the new centroids depending on the new set of samples that has been alloted to each cluster.\n",
    "- **1.5 [6 points]** `fit` : This shall contain your main loop which implements the algorithm described above. You'll sequentially call the methods above to find the $k$ centroids. Break the loop when the improvement ratio of the objective is within `rtol`. At the end (or start, depending on how you code it) of each iteration, call the method `save_plot` to save the current clustering status and save the current objective value in the `history` list.\n",
    "- **1.6 [4 points]** `predict` : Given new samples, return their assigned clusters that were learned in the `fit` step.\n",
    "\n",
    "While we're working on 2-d data (d=2) for visualization purposes, your implementation should handle any number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXjPPia0nQFz"
   },
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, k, rtol=1e-3):\n",
    "        \"\"\"\n",
    "        :param k: Number of centroids\n",
    "        :param rtol: Epsilon\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.centroids = None\n",
    "        self.snapshots = []  # buffer for progress plots\n",
    "        self.rtol = rtol\n",
    "\n",
    "    def initialize_centroids(self, X):\n",
    "        \"\"\"\n",
    "        Randomly select k **distinct** samples from the dataset in X as centroids\n",
    "        @param X: np.ndarray of dimension (num_samples, num_features)\n",
    "        @return: centroids array of shape (k, num_features)\n",
    "        \"\"\"\n",
    "        # Workspace 2.1\n",
    "        #BEGIN \n",
    "        # code here\n",
    "\n",
    "        #END\n",
    "        return centroids\n",
    "\n",
    "    def compute_distances(self, X):\n",
    "        \"\"\"\n",
    "        Compute a distance matrix of size (num_samples, k) where each cell (i, j) represents the distance between\n",
    "        i-th sample and j-th centroid. We shall use Euclidean distance here.\n",
    "        :param X: np.ndarray of shape (num_samples, num_features)\n",
    "        :return: distances_matrix : (np.ndarray) of the dimension (num_samples, k)\n",
    "        \"\"\"\n",
    "        distances_matrix = np.zeros((X.shape[0], self.k))\n",
    "        # Workspace 2.2\n",
    "        #BEGIN \n",
    "        # code here\n",
    "                \n",
    "        #END\n",
    "        return distances_matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_assignments(distances_to_centroids):\n",
    "        \"\"\"\n",
    "        Compute the assignment array of shape (num_samples,) where assignment[i] = j if and only if\n",
    "        sample i belongs to the cluster of centroid j\n",
    "        :param distances_to_centroids: The computed pairwise distances matrix of shape (num_samples, k)\n",
    "        :return: assignments array of shape (num_samples,)\n",
    "        \"\"\"\n",
    "\n",
    "        assignments = np.zeros((distances_to_centroids.shape[0],))\n",
    "\n",
    "       # Workspace 2.3\n",
    "        #BEGIN \n",
    "        # code here   \n",
    "        \n",
    "        #END\n",
    "        return assignments\n",
    "\n",
    "    def compute_centroids(self, X, assignments):\n",
    "        \"\"\"\n",
    "        Given the assignments array for the samples, compute the new centroids\n",
    "        :param X: data matrix of shape (num_samples, num_features)\n",
    "        :param assignments: array of shape (num_samples,) where assignment[i] is the current cluster of sample i\n",
    "        :return: The new centroids array of shape (k, num_features)\n",
    "        \"\"\"\n",
    "        # Workspace 2.4\n",
    "        centroids = np.zeros((self.k, X.shape[1]))\n",
    "        #BEGIN \n",
    "        # code here\n",
    "       \n",
    "        #END\n",
    "        return centroids\n",
    "\n",
    "    def compute_objective(self, X, assignments):\n",
    "        return np.sum(np.linalg.norm(X - self.centroids[assignments], axis=1) ** 2)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Implement the K-means algorithm here as described above. Loop until the improvement ratio of the objective\n",
    "        is lower than rtol. At the end of each iteration, save the k-means objective and return the objective values\n",
    "        at the end\n",
    "\n",
    "        @param X:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        self.centroids = self.initialize_centroids(X)\n",
    "        objective = np.inf\n",
    "        assignments = np.zeros((X.shape[0],))\n",
    "\n",
    "        # Workspace 1.5\n",
    "\n",
    "        while True:\n",
    "            #BEGIN \n",
    "            # code here\n",
    "            \n",
    "            #END\n",
    "        return history\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Workspace 2.6\n",
    "        assignments = np.zeros((X.shape[0],))\n",
    "        #BEGIN \n",
    "        # code here\n",
    "       \n",
    "        #END\n",
    "        return assignments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGMioJi7A4hi"
   },
   "source": [
    "### Question 6 : Explain how will the intial centroids affect the working of the K-Means algorithm. [1 point]\n",
    "\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irlgsaIfz2vX"
   },
   "outputs": [],
   "source": [
    "k_means = KMeans(5)\n",
    "objective_history = k_means.fit(multi_blobs.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCscLkq60Hu_"
   },
   "source": [
    "## Evaluating K-means\n",
    "The easiest way to evaluate the clustering quality is to use the true labels. The natural question here is: which cluster corresponds to which label?\n",
    "\n",
    "Let's first formulate the question using `multi_blobs` dataset. We have 5 clusters and 5 classes in our data. Let's create a _confusion matrix_ $C$ between the clusters and the labels so that $ C_{i,j} = \\text{size}(\\text{cluster}_i \\cap \\text{class}_j)$.\n",
    "\n",
    "We model the unknown mapping using the $5\\times 5$ boolean matrix $X$ such that $X_{i,j}=1$ if and only if $\\text{cluster}_i$ is mapped to $\\text{class}_j$.\n",
    "\n",
    "To avoid having a cluster assigned to multiple classes, each row of $X$ is constrained to have only one non-zero entry.\n",
    "\n",
    "Now, given a mapping $X$ and confusion matrix $C$, the number of correctly \"classified\" samples (not really classification, more of clustering here) is:\n",
    "\n",
    "\\begin{align}\n",
    "\\#\\text{correct} = \\sum_i \\sum_j C_{i,j} X_{i,j}\n",
    "\\end{align}\n",
    "The goal is to find $\\hat{X}$ that maximizes $\\#\\text{correct}$. To solve for $X$ we're going to use `scipy`'s `linear_sum_assignment`\n",
    "([documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_assignment.html)).\n",
    "\n",
    "## 2.7 [10 points]** Complete `evaluate_clustering` to return the accuracy of K-means using the optimal mapping $\\hat{X}$ defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HIUtYs80MYl"
   },
   "outputs": [],
   "source": [
    "def evaluate_clustering(trained_model, X, labels):\n",
    "    \"\"\"\n",
    "    Compute the ratio of correct matches between clusters from the trained model and the true labels\n",
    "    :param trained_model: Unsupervised learning model that predicts clusters\n",
    "    :param X: samples array, shape (num_samples, num_features)\n",
    "    :param labels: true labels array, shape (num_samples,\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # We can assume that the number of clusters and the number of class labels are the same\n",
    "    confusion_matrix = np.zeros((5,5))\n",
    "    boolean_matrix_X = np.zeros((5,5))\n",
    "    clusters = trained_model.predict(X)\n",
    "    # Workspace 2.7\n",
    "    #BEGIN \n",
    "    # code here\n",
    "    \n",
    "    #END\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWYsvO3xBDJd"
   },
   "source": [
    "### Question 7 : What other performance metrics can be used to evaluate K-Means performance ? [1 point]\n",
    "\n",
    "### Answer :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ontKTMx40aoZ"
   },
   "source": [
    "## 2.8 [8 points]** Run K-means on the full `multi_blobs` for 20 times. Plot the histogram (`plt.hist`) of the clustering evaluation from `evaluate_clustering`. Also report the mean clustering performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5oK0-Cz0RS7"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "accuracies = []\n",
    "# Workspace 2.8\n",
    "#BEGIN \n",
    "# code here\n",
    "\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oU9LQDJABT1C"
   },
   "source": [
    "### Question 8 : Is there any modification you can do to the K-Means to improve the performance ? If so explain. [1 point]\n",
    "\n",
    "### Answer :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10gf6ebZB_Ws"
   },
   "source": [
    "# Introduction to Eigenvectors and Eigenvalues\n",
    "\n",
    "An eigenvector of a matrix \\(A\\) is a vector that when multiplied by \\(A\\) returns a vector which is ascalar multiple of itself, i.e.,\n",
    "\n",
    "\\(Av\\)=\\(lambda v\\)\n",
    "\n",
    "Here, \\(v\\) is called the eigenvector of \\(A\\), and \\(lambda\\) is the scalar coefficient, which is called the eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hCk_vrgTCFhF",
    "outputId": "3c045fc2-e83c-4da9-9f1a-a8e3748fa68c"
   },
   "outputs": [],
   "source": [
    "A=array([[2, 1, 1], [1, 2, 1], [1, 1, 2]])\n",
    "print(np.linalg.eig(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0YwP52UIILw"
   },
   "source": [
    "Please refer to https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors , https://medium.com/sho-jp/linear-algebra-part-6-eigenvalues-and-eigenvectors-35365dc4365a for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2LTZ4goCIkz"
   },
   "source": [
    "# Covariance of a Matrix [2 points]\n",
    "\n",
    "Covariance provides the a measure of strength of correlation between two variable or more set of variables. The covariance matrix element Cij is the covariance of xi and xj. The element Cii is the variance of xi. \n",
    "\n",
    "    If COV(xi, xj) = 0 then variables are uncorrelated\n",
    "    If COV(xi, xj) > 0 then variables positively correlated\n",
    "    If COV(xi, xj) > < 0 then variables negatively correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0, 3, 4], [1, 2, 4], [3, 4, 5]])\n",
    " \n",
    "print(\"Shape of array:\\n\", np.shape(x))\n",
    " \n",
    "print(\"Covariance matrix of x:\\n\", np.cov(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2mdvOCECT4-"
   },
   "source": [
    "### Question 9 : Explain the significance of Covariance in a Dataset. [2 points]\n",
    "\n",
    "\n",
    "### ANSWER :\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XmftWU84mmq"
   },
   "source": [
    "# Principal Component Analysis [12 points]\n",
    "\n",
    "Principal component analysis, or PCA, is a powerful tool which is used to analyze data sets and is formulated in the language of linear algebra and statistics. It is an eigenvalue method used to reduce the dimension of a data set while preserving important information.\n",
    "\n",
    "Please refer to https://en.wikipedia.org/wiki/Principal_component_analysis for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0QbVxJuz0cgE",
    "outputId": "82b5ab8c-9a37-47ac-a5ca-7f6caeaf9d4f"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "\n",
    "B = np.array([[1.0,2], [3,4], [5,6]])\n",
    "\n",
    "B1 = B.copy() \n",
    "B1 -= np.mean(B1, axis=0) \n",
    "n_samples = B1.shape[0]\n",
    "print(\"B1 is B after centering:\")\n",
    "print(B1)\n",
    "\n",
    "cov_mat = np.cov(B1.T)\n",
    "pca = PCA(n_components=2) \n",
    "X = pca.fit_transform(B1)\n",
    "print(\"X\")\n",
    "print(X)\n",
    "\n",
    "eigenvecmat =   []\n",
    "print(\"Eigenvectors:\")\n",
    "for eigenvector in pca.components_:\n",
    "    if eigenvecmat == []:\n",
    "        eigenvecmat = eigenvector\n",
    "    else:\n",
    "        eigenvecmat = np.vstack((eigenvecmat, eigenvector))\n",
    "    print(eigenvector)\n",
    "print(\"eigenvector-matrix\")\n",
    "print(eigenvecmat)\n",
    "\n",
    "print(\"CHECK FOR PCA:\")\n",
    "print(\"X * eigenvector-matrix (=B1)\")\n",
    "print(np.dot(X, eigenvecmat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J36sdYO79gpm"
   },
   "source": [
    "Please refer to https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html for more details on the PCA library used above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5rVKd1l8tnl"
   },
   "source": [
    "# Using PCA for Logistic Regression [12 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metric\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARXZHBzo8D52"
   },
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "\n",
    "data = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])\n",
    "data['y'] = cancer['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNmBymTT7vZL"
   },
   "outputs": [],
   "source": [
    "group = [['mean symmetry', 'symmetry error','worst symmetry',\n",
    "'mean smoothness','smoothness error','worst smoothness']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AEEjs80h77g4",
    "outputId": "41006cc9-b71b-472c-a652-4dc60796db6c"
   },
   "outputs": [],
   "source": [
    "for i,g in enumerate(group):\n",
    "\n",
    "    x = data[g]\n",
    "    x = sm.add_constant(x)\n",
    "    y = data['y']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3, \n",
    "                                                        random_state = 101)\n",
    "\n",
    "    model = sm.Logit(y_train,x_train).fit() #fit logistic regression model\n",
    "\n",
    "    predictions = np.around(model.predict(x_test))\n",
    "    accuracy = metric.accuracy_score(y_test,predictions)\n",
    "    \n",
    "    print(\"Accuracy of Group {}: {}\".format(i+1,accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bhrNYBP8xLC"
   },
   "source": [
    "## Use PCA on the above data and fit the data into a Logistic Regression Model. [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pq5UWkB_Ch0Q"
   },
   "source": [
    "### Question 10 : Was there any improvement on the performance of the Logistic Regression model after applying PCA on the data ? If so why ? [2 points]\n",
    "\n",
    "### Answer : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-NP6r4JPa9S"
   },
   "source": [
    "### **Finally:**\n",
    "    \n",
    "What are your thoughts about Matrix Factorization, PCA and K-Means from this project? \n",
    "Which model was the simpliest?\n",
    "Which expressed the underlying ideas best?\n",
    "As we use more packages for mathematical modelling do we sometimes lose track of the ideas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer :"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LinearAlgebraProject.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
